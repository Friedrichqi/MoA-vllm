import argparse
import asyncio
import json
from typing import Dict
import httpx
import os
import pdb
from tqdm import tqdm
import torch
import torch.multiprocessing as mp

async def complete_prompt(
    base_url: str,
    model: str,
    prompt: str,
    temperature: float = 0.7,
    max_tokens: int = 1024,
    top_p: float = 0.9,
) -> str:
    payload = {
        "model": model,
        "prompt": prompt,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": False,
        "top_p": top_p,
        "repetition_penalty": 1.05,
    }
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer EMPTY",
    }
    async with httpx.AsyncClient(timeout=1200) as client:
        resp = await client.post(f"{base_url}/v1/completions", headers=headers, json=payload)
        resp.raise_for_status()
        return resp.json()["choices"][0]["text"].strip()

async def async_query(model_name, prompt, url, temperature=0.7, top_p=0.9, max_tokens=1024):
    return model_name, await complete_prompt(url, model_name, prompt, temperature, max_tokens, top_p)

async def process_stage(stage_name, stage_config, input_prompt, model2port, initial_context, show_intermediates):
    """Helper function to run a single stage (proposer or aggregator)."""
    # print(f"\nâ–¸ Running {stage_name} stage...")

    last_layer_output = initial_context
    stage_output_history = []

    for layer_name, layer_configs in stage_config.items():
        tasks = []
        for _, config in layer_configs.items():
            model_name = config["model"]
            url = f"http://localhost:{model2port[model_name]}"
            temperature = config.get("temperature", 0.7)
            top_p = config.get("top_p", 0.9)
            max_tokens = config.get("max_tokens", 1024)
            
            prompt = config["prompt"].replace("[input_prompts]", input_prompt)
            prompt = prompt.replace("[responses]", last_layer_output)

            tasks.append(async_query(model_name, prompt, url, temperature, top_p, max_tokens))

        layer_results = await asyncio.gather(*tasks)
        stage_output_history.append((layer_name, layer_results))
        
        # Concatenate results from the current layer to be used as context for the next layer.
        texts = [text for _, text in layer_results]
        last_layer_output = "".join(texts)

    if show_intermediates:
        print(f"\n--- {stage_name.capitalize()} Intermediate Results ---")
        for layer_name, results in stage_output_history:
            print(f"\n--- {layer_name} ---")
            for name, text in results:
                print(f"\n--- {name} says ---\n{text}")
        print("-" * (len(stage_name) + 28))

    return last_layer_output

async def run_moa(
    input_prompt: str,
    show_intermediates: bool,
    proposer_config: Dict[str, Dict[str, Dict]],
    aggregator_config: Dict[str, Dict[str, Dict]],
):
    """
    Runs the Mixture-of-Agents (MoA) pipeline.
    """

    with open("model2port.json", "r") as f:
        model2port = json.load(f)
    
    # Stage 1: Proposers
    proposer_output = await process_stage(
        stage_name="proposer",
        stage_config=proposer_config,
        input_prompt=input_prompt,
        model2port=model2port,
        initial_context="",
        show_intermediates=show_intermediates
    )

    # Stage 2: Aggregators
    aggregator_output = await process_stage(
        stage_name="aggregator",
        stage_config=aggregator_config,
        input_prompt=input_prompt,
        model2port=model2port,
        initial_context=proposer_output,
        show_intermediates=show_intermediates
    )

    return aggregator_output

def process_entry(args_tuple):
    entry, args, few_shots, configs = args_tuple
    question = entry["question"]
    golden_answer = entry["answer"]

    # Adding few-shot learning samples to the prompt
    if args.few_shots > 0:
        fs_question = "".join([f"{fs['question']} {fs['answer']} " for fs in few_shots]) + ". " + question
        input_prompt = fs_question.strip()
    else:
        input_prompt = question.strip()

    # Run MoA with the given prompt
    final_answer = asyncio.run(
        run_moa(input_prompt=input_prompt, show_intermediates=False, proposer_config=configs["proposer"], aggregator_config=configs["aggregator"])
    )
    return {"question": question, "answer": final_answer, "golden_answer": golden_answer}

def main(args):
    # Loading MoA configs generated by 'start_server.py'
    with open("configs.json", "r") as f:
        configs = json.load(f)

    # Fetching test prompts
    with open(os.path.join("benchmark", f"{args.benchmark}_test.jsonl")) as f:
        test_entries = [json.loads(line) for line in f.readlines()]
    
    # Removing repetitive questions
    os.makedirs("output", exist_ok=True)
    output_path = os.path.join("output", f"{args.benchmark}_fs{args.few_shots}.jsonl")
    existing_questions = set()
    if os.path.exists(output_path):
        with open(output_path, "r") as f:
            for line in f:
                existing_questions.add(json.loads(line)["question"])
    entries_to_process = [entry for entry in test_entries if entry["question"] not in existing_questions]
    
    if not entries_to_process:
        print("All questions have already been processed.")
        return

    # Fetching few-shot learning samples
    if args.few_shots > 0:
        with open(os.path.join("benchmark", f"{args.benchmark}_train.jsonl")) as f:
            few_shots = []
            for id, line in enumerate(f.readlines()):
                if id >= args.few_shots:
                    break
                few_shots.append(json.loads(line))
    
    # Prepare arguments for multiprocessing
    args_list = [(entry, args, few_shots, configs) for entry in entries_to_process]
    
    # Beginning of evaluation
    with mp.Pool(args.concurrent) as pool:
        with open(output_path, "a") as f:
            for result in tqdm(pool.imap_unordered(process_entry, args_list), total=len(entries_to_process)):
                f.write(json.dumps(result) + "\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate Mixture-of-Agents on different benchmarks.")
    parser.add_argument("--benchmark", "-b", required=True, help="Which benchmark we use to evaluate")
    parser.add_argument("--few_shots", type=int, default=0, help="Number of few-shot learning samples")
    parser.add_argument("--concurrent", type=int, default=1, help="Number of concurrent instances to run")
    args = parser.parse_args()
    main(args)